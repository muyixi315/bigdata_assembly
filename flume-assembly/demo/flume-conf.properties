# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.


# The configuration file needs to define the sources,
# the channels and the sinks.
# Sources, channels and sinks are defined per #agent,
# in this case called '#agent'

#agent.sources = seqGenSrc
#agent.channels = memoryChannel
#agent.sinks = loggerSink

# For each one of the sources, the type is defined
#agent.sources.seqGenSrc.type = seq

# The channel can be defined as follows.
#agent.sources.seqGenSrc.channels = memoryChannel

# Each sink's type must be defined
#agent.sinks.loggerSink.type = logger

#Specify the channel the sink should use
#agent.sinks.loggerSink.channel = memoryChannel

# Each channel's type is defined.
#agent.channels.memoryChannel.type = memory

# Other config values specific to each type of channel(sink or source)
# can be defined as well
# In this case, it specifies the capacity of the memory channel
#agent.channels.memoryChannel.capacity = 100


# example.conf: A single-node Flume configuration

# Name the components on this agent
agent.sources = r1 r2 r3 r4 
agent.sinks = es_sink es_sink2 es_sink3 es_sink4
agent.channels = c1 c2 c3 c4

# Describe/configure the source

agent.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.r1.zookeeperConnect = liebao49.test.com:2181,liebao235.test.com:2181,liebao36.test.com:2181
agent.sources.r1.topic =mouth
agent.sources.r1.groupId = flume
agent.sources.r1.kafka.consumer.timeout.ms = 100

# Describe the sink

agent.sinks.es_sink.type=com.cognitree.flume.sink.elasticsearch.ElasticSearchSink
agent.sinks.es_sink.es.bulkActions=5
agent.sinks.es_sink.es.bulkProcessor.name=bulkprocessor
agent.sinks.es_sink.es.bulkSize=5
agent.sinks.es_sink.es.bulkSize.unit=MB
agent.sinks.es_sink.es.concurrent.request=1
agent.sinks.es_sink.es.flush.interval.time=5m
agent.sinks.es_sink.es.backoff.policy.time.interval=50M
agent.sinks.es_sink.es.backoff.policy.retries=8
agent.sinks.es_sink.es.cluster.name=mycluster
agent.sinks.es_sink.es.client.transport.sniff=false
agent.sinks.es_sink.es.client.transport.ignore_cluster_name=false
agent.sinks.es_sink.es.client.transport.ping_timeout=5s
agent.sinks.es_sink.es.client.transport.nodes_sampler_interval=5s
agent.sinks.es_sink.es.client.hosts=liebao99.test.com
agent.sinks.es_sink.es.client.port=9300
agent.sinks.es_sink.es.index=mouth
agent.sinks.es_sink.es.type=mouth-%y-%m-%d
agent.sinks.es_sink.es.index.builder=com.cognitree.flume.sink.elasticsearch.HeaderBasedIndexBuilder
agent.sinks.es_sink.es.serializer=com.cognitree.flume.sink.elasticsearch.SimpleSerializer
agent.sinks.es_sink.es.serializer.csv.fields=id:int,name:string,isemployee:boolean,leaves:float
agent.sinks.es_sink.es.serializer.csv.delimiter=,
agent.sinks.es_sink.es.serializer.avro.schema.file=/usr/local/schema.avsc

# Use a channel which buffers events in memory
agent.channels.c1.type = memory
agent.channels.c1.capacity = 100000
agent.channels.c1.transactionCapacity = 10000

# Bind the source and sink to the channel
agent.sources.r1.channels = c1
agent.sinks.es_sink.channel = c1




# Describe/configure the source

agent.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.r2.zookeeperConnect = liebao49.test.com:2181,liebao235.test.com:2181,liebao36.test.com:2181
agent.sources.r2.topic =parameters
agent.sources.r2.groupId = flume
agent.sources.r2.kafka.consumer.timeout.ms = 100

# Describe the sink

agent.sinks.es_sink2.type=com.cognitree.flume.sink.elasticsearch.ElasticSearchSink
agent.sinks.es_sink2.es.bulkActions=5
agent.sinks.es_sink2.es.bulkProcessor.name=bulkprocessor
agent.sinks.es_sink2.es.bulkSize=5
agent.sinks.es_sink2.es.bulkSize.unit=MB
agent.sinks.es_sink2.es.concurrent.request=1
agent.sinks.es_sink2.es.flush.interval.time=5m
agent.sinks.es_sink2.es.backoff.policy.time.interval=50M
agent.sinks.es_sink2.es.backoff.policy.retries=8
agent.sinks.es_sink2.es.cluster.name=mycluster
agent.sinks.es_sink2.es.client.transport.sniff=false
agent.sinks.es_sink2.es.client.transport.ignore_cluster_name=false
agent.sinks.es_sink2.es.client.transport.ping_timeout=5s
agent.sinks.es_sink2.es.client.transport.nodes_sampler_interval=5s
agent.sinks.es_sink2.es.client.hosts=liebao99.test.com
agent.sinks.es_sink2.es.client.port=9300
agent.sinks.es_sink2.es.index=parameters
agent.sinks.es_sink2.es.type=parameters-%y-%m-%d
agent.sinks.es_sink2.es.index.builder=com.cognitree.flume.sink.elasticsearch.HeaderBasedIndexBuilder
agent.sinks.es_sink2.es.serializer=com.cognitree.flume.sink.elasticsearch.SimpleSerializer
agent.sinks.es_sink2.es.serializer.csv.fields=id:int,name:string,isemployee:boolean,leaves:float
agent.sinks.es_sink2.es.serializer.csv.delimiter=,
agent.sinks.es_sink2.es.serializer.avro.schema.file=/usr/local/schema.avsc

# Use a channel which buffers events in memory
agent.channels.c2.type = memory
agent.channels.c2.capacity = 100000
agent.channels.c2.transactionCapacity = 10000

# Bind the source and sink to the channel
agent.sources.r2.channels = c2
agent.sinks.es_sink2.channel = c2

# Describe/configure the source

agent.sources.r3.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.r3.zookeeperConnect = liebao49.test.com:2181,liebao235.test.com:2181,liebao36.test.com:2181
agent.sources.r3.topic =publicSentiment1
agent.sources.r3.groupId = flume
agent.sources.r3.kafka.consumer.timeout.ms = 100

# Describe the sink

agent.sinks.es_sink3.type=com.cognitree.flume.sink.elasticsearch.ElasticSearchSink
agent.sinks.es_sink3.es.bulkActions=5
agent.sinks.es_sink3.es.bulkProcessor.name=bulkprocessor
agent.sinks.es_sink3.es.bulkSize=5
agent.sinks.es_sink3.es.bulkSize.unit=MB
agent.sinks.es_sink3.es.concurrent.request=1
agent.sinks.es_sink3.es.flush.interval.time=5m
agent.sinks.es_sink3.es.backoff.policy.time.interval=50M
agent.sinks.es_sink3.es.backoff.policy.retries=8
agent.sinks.es_sink3.es.cluster.name=mycluster
agent.sinks.es_sink3.es.client.transport.sniff=false
agent.sinks.es_sink3.es.client.transport.ignore_cluster_name=false
agent.sinks.es_sink3.es.client.transport.ping_timeout=5s
agent.sinks.es_sink3.es.client.transport.nodes_sampler_interval=5s
agent.sinks.es_sink3.es.client.hosts=liebao99.test.com
agent.sinks.es_sink3.es.client.port=9300
agent.sinks.es_sink3.es.index=publicSentiment1
agent.sinks.es_sink3.es.type=publicSentiment1-%y-%m-%d
agent.sinks.es_sink3.es.index.builder=com.cognitree.flume.sink.elasticsearch.HeaderBasedIndexBuilder
agent.sinks.es_sink3.es.serializer=com.cognitree.flume.sink.elasticsearch.SimpleSerializer
agent.sinks.es_sink3.es.serializer.csv.fields=id:int,name:string,isemployee:boolean,leaves:float
agent.sinks.es_sink3.es.serializer.csv.delimiter=,
agent.sinks.es_sink3.es.serializer.avro.schema.file=/usr/local/schema.avsc

# Use a channel which buffers events in memory
agent.channels.c3.type = memory
agent.channels.c3.capacity = 100000
agent.channels.c3.transactionCapacity = 10000

# Bind the source and sink to the channel
agent.sources.r3.channels = c3
agent.sinks.es_sink3.channel = c3


# Describe/configure the source

agent.sources.r4.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.r4.zookeeperConnect = liebao49.test.com:2181,liebao235.test.com:2181,liebao36.test.com:2181
agent.sources.r4.topic =dealer
agent.sources.r4.groupId = flume
agent.sources.r4.kafka.consumer.timeout.ms = 100
agent.sources.r4.interceptors = i1
agent.sources.r4.interceptors.i1.type = com.cognitree.flume.sink.elasticsearch.filter.BodyGetIdInterceptor$Builder
agent.sources.r4.interceptors.i1.primaryKey = data.DealerId

# Describe the sink

agent.sinks.es_sink4.type=com.cognitree.flume.sink.elasticsearch.ElasticSearchSink
agent.sinks.es_sink4.es.bulkActions=5
agent.sinks.es_sink4.es.bulkProcessor.name=bulkprocessor
agent.sinks.es_sink4.es.bulkSize=5
agent.sinks.es_sink4.es.bulkSize.unit=MB
agent.sinks.es_sink4.es.concurrent.request=1
agent.sinks.es_sink4.es.flush.interval.time=5m
agent.sinks.es_sink4.es.backoff.policy.time.interval=50M
agent.sinks.es_sink4.es.backoff.policy.retries=8
agent.sinks.es_sink4.es.cluster.name=mycluster
agent.sinks.es_sink4.es.client.transport.sniff=false
agent.sinks.es_sink4.es.client.transport.ignore_cluster_name=false
agent.sinks.es_sink4.es.client.transport.ping_timeout=5s
agent.sinks.es_sink4.es.client.transport.nodes_sampler_interval=5s
agent.sinks.es_sink4.es.client.hosts=liebao99.test.com
agent.sinks.es_sink4.es.client.port=9300
agent.sinks.es_sink4.es.index=dealer
agent.sinks.es_sink4.es.type=dealer-%y-%m
agent.sinks.es_sink4.es.index.builder=com.cognitree.flume.sink.elasticsearch.HeaderBasedIndexBuilder
agent.sinks.es_sink4.es.serializer=com.cognitree.flume.sink.elasticsearch.SimpleSerializer
agent.sinks.es_sink4.es.serializer.csv.fields=id:int,name:string,isemployee:boolean,leaves:float
agent.sinks.es_sink4.es.serializer.csv.delimiter=,
agent.sinks.es_sink4.es.serializer.avro.schema.file=/usr/local/schema.avsc

# Use a channel which buffers events in memory
agent.channels.c4.type = memory
agent.channels.c4.capacity = 100000
agent.channels.c4.transactionCapacity = 10000

# Bind the source and sink to the channel
agent.sources.r4.channels = c4
agent.sinks.es_sink4.channel = c4